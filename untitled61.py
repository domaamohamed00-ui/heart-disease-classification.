# -*- coding: utf-8 -*-
"""Untitled61.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RrTTpcbDAnaKXJ24DFGF5aRKaEMNSENZ
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np # Added for numpy operations
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression # Added for Logistic Regression
from sklearn.neighbors import KNeighborsClassifier # Added for KNN Classifier
from sklearn.metrics import confusion_matrix # Added for confusion_matrix

accuracies = {} # Initialized accuracies dictionary

df = pd.read_csv('/content/heart.csv')

df.head()

df.info()

df.shape

df.isnull().sum()

df.duplicated().sum()

df.drop_duplicates(inplace=True, keep = 'first')

df.shape

df.describe().T

df['target'].value_counts()

df = df[df['ca'] < 4]  #CA IN KAGGLE 0<3 delete any num bigger than 3
DF = df[df['thal'] > 0] #THAL IN KAGGLE delete any num less than 0
print(df.shape)
print(f'len of data: {len(df)}')

df.columns

df = df.rename(
    columns = {
        'cp' : 'chest_pain_type',
        'trestbps':'resting_blood_pressure',
               'chol': 'cholesterol',
               'fbs': 'fasting_blood_sugar',
               'restecg' : 'resting_electrocardiogram',
               'thalach': 'max_heart_rate_achieved',
               'exang': 'exercise_induced_angina',
               'oldpeak': 'st_depression',
               'slope': 'st_slope',
               'ca':'num_major_vessels',
               'thal': 'thalassemia'},
    errors = "raise")

df.head()

df['sex'][df['sex'] == 0] = 'female'
df['sex'][df['sex'] == 1] = 'male'

df.dtype

count = 0
for i in df.dtypes
if i == object:
  count += 1

print(count)

df.describe(include='object')

get_object_columns = [col for col in df.columns if df[col].dtypes == 'object']
get_object_columns

plt.figure(figsize = (16,10))
for i in get_object_columns:
    sns.catplot(y = "target", x = i, hue = "sex",data = df, kind = "bar",
               palette={"male": "blue", "female": "pink"})

def label_encode_cat_features(data, cat_features):
  label_encoder = LabelEncoder()
  data_encoded = data.copy()

  for col in cat_features:
    data_encoded[col] = label_encoder.fit_transform(data[col])
  data = data_encoded
  return data

cat_features = ['sex', 'fasting_blood_sugar', 'exercise_induced_angina', 'target',
                'chest_pain_type', 'resting_electrocardiogram', 'st_slope', 'thalassemia']

df = label_encode_cat_features(df, cat_features)

df.head()

X = df.iloc[ : , :-1]
y = df.iloc[ : , -1]

X.head(2)

y.head(2)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, shuffle = True)

lr = LogisticRegression()
lr.fit(X_train,y_train)
acc = lr.score(X_test,y_test)*100

accuracies['Logistic Regression'] = acc
print("Test Accuracy {:.2f}%".format(acc))

scoreList = []
for i in range(1,20):
    knn = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k
    knn.fit(X_train, y_train)
    scoreList.append(knn.score(X_test, y_test))

plt.plot(range(1,20), scoreList)
plt.xticks(np.arange(1,20,1))
plt.xlabel("K value")
plt.ylabel("Score")
plt.show()

acc = max(scoreList)*100
accuracies['KNN'] = acc
print("Maximum KNN Score is {:.2f}%".format(acc))

accuracies

from sklearn.svm import SVC

svc = SVC()
svc.fit(X_train, y_train)

acc = svc.score(X_test,y_test)*100
accuracies['SVC'] = acc
print(f"Test Accuracy of SVC Algorithm: {acc:.2f}%")

from sklearn.naive_bayes import GaussianNB
nb = GaussianNB()
nb.fit(X_train, y_train)

acc = nb.score(X_test,y_test)*100
accuracies['Naive Bayes'] = acc
print(f"Accuracy of Naive Bayes: {acc:.2f}%")

from sklearn.tree import DecisionTreeClassifier
dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)

acc = dtc.score(X_test, y_test)*100
accuracies['Decision Tree'] = acc
print(f"Decision Tree Test Accuracy {acc:.2f}%")

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators = 1000, random_state = 1)
rf.fit(X_train, y_train)

acc = rf.score(X_test,y_test)*100
accuracies['Random Forest'] = acc
print(f"Random Forest Algorithm Accuracy Score : {acc:.2f}%")

colors = ["skyblue", "salmon", "lightgreen", "gold", "lightcoral", "plum"]

sns.set_style("whitegrid")
plt.figure(figsize=(16, 5))
plt.yticks(np.arange(0, 110, 10))
plt.ylabel("Accuracy %")
plt.xlabel("Algorithms")


ax = sns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)


for i, v in enumerate(accuracies.values()):
    ax.text(i, v + 1, f"{v:.2f}%", color='black', ha='center', fontweight='bold')

plt.title("Model Accuracy Comparison")
plt.ylim(0, 110)
plt.show()

y_pred_lr  = lr.predict(X_test)
y_pred_knn = knn.predict(X_test)
y_pred_svc = svc.predict(X_test)
y_pred_nb  = nb.predict(X_test)
y_pred_dtc = dtc.predict(X_test)
y_pred_rf  = rf.predict(X_test)

cm_lr = confusion_matrix(y_test,y_pred_lr)
cm_knn = confusion_matrix(y_test,y_pred_knn)
cm_svc = confusion_matrix(y_test,y_pred_svc)
cm_nb = confusion_matrix(y_test,y_pred_nb)
cm_dtc = confusion_matrix(y_test,y_pred_dtc)
cm_rf = confusion_matrix(y_test,y_pred_rf)

models = [
    ("Logistic Regression", cm_lr),
    ("K Nearest Neighbors", cm_knn),
    ("Support Vector Machine", cm_svc),
    ("Naive Bayes", cm_nb),
    ("Decision Tree Classifier", cm_dtc),
    ("Random Forest", cm_rf)
]

plt.figure(figsize=(24, 12))
plt.suptitle("Confusion Matrixes", fontsize=24)
plt.subplots_adjust(wspace=0.4, hspace=0.4)

for i, (title, cm) in enumerate(models, 1):
    plt.subplot(2, 3, i)
    plt.title(f"{title} Confusion Matrix")
    sns.heatmap(cm, annot=True, cmap="Blues", fmt="d", cbar=False, annot_kws={"size": 24})

plt.show()